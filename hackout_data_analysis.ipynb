{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers sentence-transformers pyngrok"
      ],
      "metadata": {
        "id": "ua3HVHHkVnC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tyQbKNBeVVIZ"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "embedding_model = SentenceTransformer('BAAI/bge-small-en-v1.5')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "from pyngrok import ngrok\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def get_embeddings(embedding_model,words):\n",
        "    embeddings = embedding_model.encode(words)\n",
        "    df = pd.DataFrame(embeddings,columns = [f\"feat_{i}\" for i in range(384)])\n",
        "    return df\n",
        "\n",
        "threshold = 0.7  # Adjust as needed\n",
        "\n",
        "def combine_rows_based_on_cosine_similarity(df, threshold):\n",
        "    num_rows = df.shape[0]\n",
        "    combined = set()\n",
        "\n",
        "    for i in range(num_rows):\n",
        "        if i not in combined:\n",
        "            for j in range(i + 1, num_rows):\n",
        "                if j not in combined:\n",
        "                    i_vector = df.iloc[i, :].values.reshape(1, -1)\n",
        "                    j_vector = df.iloc[j, :].values.reshape(1, -1)\n",
        "                    sim = cosine_similarity(i_vector, j_vector)[0][0]\n",
        "                    if sim > threshold:\n",
        "                        combined.add(j)\n",
        "\n",
        "    return df.drop(index=list(combined))\n",
        "\n",
        "def combine_similar_words_to_unique(result_df,embedding_df,words):\n",
        "    unique_words = set()\n",
        "    for sentence, embedding in zip(words, embedding_df.values):\n",
        "        exists = (result_df==embedding).all(axis=1).any()\n",
        "\n",
        "        if exists:\n",
        "            unique_words.add(sentence)\n",
        "\n",
        "    unique_words = list(unique_words)\n",
        "    return unique_words"
      ],
      "metadata": {
        "id": "qxSGZcmRVcHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from transformers import (\n",
        "    TokenClassificationPipeline,\n",
        "    AutoModelForTokenClassification,\n",
        "    AutoTokenizer,\n",
        ")\n",
        "from transformers.pipelines import AggregationStrategy\n",
        "import numpy as np\n",
        "import json"
      ],
      "metadata": {
        "id": "w0Z0KddHVcFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define keyphrase extraction pipeline\n",
        "class KeyphraseExtractionPipeline(TokenClassificationPipeline):\n",
        "    def __init__(self, model, *args, **kwargs):\n",
        "        super().__init__(\n",
        "            model=AutoModelForTokenClassification.from_pretrained(model),\n",
        "            tokenizer=AutoTokenizer.from_pretrained(model),\n",
        "            *args,\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "    def postprocess(self, all_outputs):\n",
        "        results = super().postprocess(\n",
        "            all_outputs=all_outputs,\n",
        "            aggregation_strategy=AggregationStrategy.SIMPLE,\n",
        "        )\n",
        "        return np.unique([result.get(\"word\").strip() for result in results])\n",
        "\n",
        "model_name = \"ml6team/keyphrase-extraction-kbir-inspec\"\n",
        "extractor = KeyphraseExtractionPipeline(model=model_name)\n",
        "\n",
        "def measure_user_intent(query,extractor):\n",
        "    # Load pipeline\n",
        "\n",
        "    keyphrases = extractor(f\"\"\"{query}\"\"\".replace(\"\\n\", \" \"))\n",
        "    return keyphrases"
      ],
      "metadata": {
        "id": "PZy3JconVcDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app = Flask(__name__)\n",
        "port_no = 3000  # Change this to your desired port number\n",
        "\n",
        "# Configure the ngrok tunnel directly within the script\n",
        "ngrok.set_auth_token(\"2Xi0Us7DYzDIYk1I5wjI2Ky9aKd_7bXnTTPPy2cT5znmK9zY5\")\n",
        "\n",
        "# Define the ngrok tunnel configuration\n",
        "public_url = ngrok.connect(port_no, proto=\"http\", bind_tls=True)\n",
        "\n",
        "@app.route('/', methods=['POST'])\n",
        "def home():\n",
        "\n",
        "    if request.method == 'POST':\n",
        "        query = request.form['query']\n",
        "        preexisting_words = request.form['words']\n",
        "\n",
        "        prex_words = preexisting_words.split(\";\")\n",
        "\n",
        "        words = measure_user_intent(query,extractor).tolist()\n",
        "        embedings_df = get_embeddings(embedding_model,words)\n",
        "        prex_embeddings_df = get_embeddings(embedding_model,prex_words)\n",
        "        total_df = pd.concat([prex_embeddings_df,embeddings_df])\n",
        "        result_df = combine_rows_based_on_cosine_similarity(total_df, threshold)\n",
        "        unique_words = combine_similar_words_to_unique(result_df,embeddings_df,words)\n",
        "\n",
        "        return jsonify({\"result\":unique_words})\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(f\"To access the Global link, please click {public_url.public_url}\")\n",
        "    app.run(port=port_no)"
      ],
      "metadata": {
        "id": "c3uqbOPQVcBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sY9zAZKRVb_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BuJK9tjGVb9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CA1l1TiFVb7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zx6ibDlBVb5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VOmKjPGKVb3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8vbMDGmpVb1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wrjwscOBVbzU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}